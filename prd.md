# Joshua PowerPoint – Product Requirements Document

## Overview

**Joshua PowerPoint** is an AI-driven application that generates complete PowerPoint presentations from a user’s prompt or brief. It addresses the time-consuming task of creating slide decks by automating content creation (text and imagery) and slide layout assembly. The product is intended for professionals, educators, and anyone who needs to produce presentation slides quickly and with minimal manual effort, without sacrificing quality or creativity. Drawing on a powerful Large Language Model (LLM) and image generation service, Joshua PowerPoint can produce a structured slide outline and then turn it into a downloadable **.pptx** file. This delivers substantial value by saving users time in brainstorming content, designing slides, and searching for images, allowing them to focus on refining messaging rather than building slides from scratch. By leveraging AI to generate slide content and design, the application accelerates the creation of presentation materials while maintaining consistency and professionalism. Users benefit from an interactive chat-style interface to iteratively refine the presentation content, and the final output is a ready-to-use PowerPoint file – achieving the goal of going from *prompt to PPTX* with speed and ease. The approach has been validated with performance targets (e.g. generating a 5-slide deck in under 30 seconds), ensuring the solution is not only convenient but also efficient.

## Core Features

Joshua PowerPoint’s functionality spans both frontend user experience features and backend AI/processing capabilities. The core features include:

* **AI-Powered Slide Outline Generation:** The system can generate structured slide outlines based on a user’s prompt. **What it does:** Accepts a text prompt (and parameters like desired number of slides or an AI model choice) and returns a list of slides each with a title and bullet points (the “slide plan”). **Why it’s important:** This eliminates the need for users to manually brainstorm or organise presentation content – the AI provides a coherent starting structure. **How it works:** The frontend captures the user’s prompt and sends it via a `/chat/generate` API call to the backend FastAPI service. The backend’s LLM Service (integrated with OpenRouter) processes the prompt and returns a list of slide plans in a structured format. Under the hood, a Pydantic model `ChatRequest` (including fields like prompt text, slide count, and model name) is validated and passed to the LLM API, and the response is parsed into a list of `SlidePlan` objects. If the external LLM API responds with an error or invalid data, the service handles it by raising appropriate errors (e.g. HTTP 502 on upstream failures). This feature provides the “brain” of the application – quickly generating an initial draft outline for the presentation content.

* **AI-Generated Imagery for Slides:** The application auto-generates illustrative images for the slides to complement the text content. **What it does:** For each slide in the outline, an image relevant to the slide’s content is fetched or generated using an AI image service. **Why it’s important:** High-quality relevant images enhance presentations, but finding or creating them is tedious – this feature automates that process, ensuring each slide has a visual component without the user manually searching. **How it works:** The backend includes an **Image Service** module that interfaces with the *Runware* API (a third-party image generation service). When a slide outline is ready, the backend asynchronously calls `generate_images(slides: list[SlidePlan])` which returns a list of `ImageMeta` objects (each containing image URL or path). The image service uses retry logic for robustness (up to 3 retries on failures), and any missing image is handled by substituting a branded placeholder graphic. This ensures that by the time slide assembly happens, there’s either a relevant image or a fallback for every slide. The inclusion of auto-generated images greatly enriches the final presentation’s visual appeal.

* **Automatic PowerPoint Deck Assembly (PPTX Generation):** The system compiles the text outline and images into a fully formatted PowerPoint `.pptx` file. **What it does:** Creates a PowerPoint file with slides populated by the generated titles, bullet points, and images, applying a consistent template/style. **Why it’s important:** Users receive a ready-to-use PowerPoint file that they can download and edit or present, eliminating the manual work of copy-pasting content into PowerPoint. **How it works:** The backend’s **PPTX Builder** module uses the `python-pptx` library to programmatically construct the presentation. Once the slide outline and images are prepared, this service iterates over each slide, adding a title text box, body content (bullet points), and inserting the slide image (resizing or positioning as needed), and even speaker notes if provided. The slides use a master template for consistent styling. The resulting PowerPoint file is written to a temporary location (e.g. `/tmp/{uuid}.pptx`). In case an image is missing or failed to generate, the builder inserts a default placeholder image to maintain the slide’s layout. This feature automates what would traditionally be a manual design task, ensuring the output is in the ubiquitous PowerPoint format (.pptx) ready for the user.

* **Interactive Chat-Based UI for Refinement:** Users interact with Joshua PowerPoint through a real-time chat interface to generate and refine their presentation content. **What it does:** Provides a conversational UI in which the user can enter their initial prompt and then iteratively refine the results (e.g. asking the AI to adjust a slide, add content, or regenerate with a different tone or model). It also displays the generated slide outline within the chat for review. **Why it’s important:** This iterative approach makes the content generation process flexible and user-friendly – users are not stuck with a one-shot output and can steer the AI if the outline isn’t exactly what they want. **How it works:** On the frontend, a **Chat UI Module** manages this experience. It uses a combination of React state (via Zustand stores) and server interactions. When a user enters a prompt, the chat interface records the input and triggers a TanStack Query mutation to call the backend (`/chat/generate`). The UI is designed to update reactively: once the backend responds with the slide plan, the outline is stored in state and the UI re-renders to display the proposed slides. The chat history (prompts and responses) is maintained in a chat transcript for context, allowing follow-up prompts to be sent along with prior context if needed (a `sessionId` is used to identify the conversation). This feature is supported by a **Model Selector** component allowing the user to choose from different AI models for generation, and a **ChatInput** for entering messages, etc. The chat interface thus acts as both input method and a preview of the AI’s output (slide outline) in real-time, making the process feel like conversing with a presentation assistant.

* **Real-Time Slide Generation Progress (WebSockets):** Joshua PowerPoint provides live feedback on the slide deck creation process, particularly when assembling the final PPTX. **What it does:** As the system generates slides and builds the PowerPoint file, it sends real-time progress updates to the frontend (e.g. “Slide 3 of 5 completed”) and notifies when the final deck is ready for download. **Why it’s important:** Generating an entire presentation (especially with images and PPT rendering) can take some time. Real-time feedback keeps the user informed that the process is active and how far along it is, improving transparency and user experience. **How it works:** The backend uses a WebSocket server (powered by `python-socketio` integrated with FastAPI) to push events to the client. When the user initiates slide deck generation (e.g. by confirming the outline and requesting the PPT), the request `/slides/build` starts an asynchronous job (potentially dispatched to a Celery worker in production) to generate images and assemble the PPTX. As each slide is processed, the backend emits a `slide:progress` event over the WebSocket channel with data like `{ slideIndex: n }` indicating how many slides are done. Once the PowerPoint file is fully generated and saved, a `slide:completed` event is emitted with a payload containing a `pptxUrl`. The frontend, which maintains a Socket.IO connection via a custom React hook (`useWebSocket`), listens for these events. Upon receiving `slide:completed`, the UI can display a “Download ready” prompt or automatically start the download. This real-time feedback loop is core to keeping users engaged during the processing delay and ensures they can retrieve the result the moment it’s available.

* **Secure Download & Export:** The final PowerPoint file can be downloaded by the user through a secure, resumable link. **What it does:** Allows the user to fetch the generated `.pptx` file once ready, via a link that’s time-limited for security. Supports resuming partial downloads to handle network hiccups. **Why it’s important:** Ensures the potentially large PPTX file is delivered reliably to the user without requiring them to navigate to any external system. Also protects the download endpoint from abuse by using pre-signed URLs or token-based access. **How it works:** When the slide generation is complete, the backend either stores the file in an object storage (e.g. S3 or similar) and generates a pre-signed URL, or serves it through an API endpoint with a short-lived token. The `slide:completed` WebSocket event carries this URL (`pptxUrl`) for the frontend. The frontend’s **Download Manager** component handles the actual file retrieval, using HTTP GET requests with support for range headers (allowing resume). If the pre-signed URL expires or the download fails mid-way, the Download Manager can request a new URL or retry the download segment, ensuring the user eventually gets the full file. This feature abstracts away the complexity of file transfer from the user – they simply click “Download” and the system handles the rest, providing the PPTX file for local use.

* **User Accounts & Authentication:** *(Planned core feature)* The system supports a basic user authentication mechanism to secure access and potentially allow saving of sessions or usage tracking. **What it does:** Allows users to log in without passwords (via magic email links) and issues a JSON Web Token (JWT) for session authentication. **Why it’s important:** If the service is offered to a controlled group (e.g. within an organisation or a beta program), authentication ensures only authorised users access it. It also lays groundwork for future features like saving past presentations or user-specific settings. **How it works:** The backend provides an `/auth` API with endpoints like `/auth/login` (to request a magic link by email) and `/auth/refresh` (to renew tokens), as well as uses JWT in headers for protected calls. When implemented, a user would enter their email, receive an email with a secure link, and clicking it would log them in by retrieving a signed JWT. The FastAPI app integrates JWT verification in protected routes using dependency injections. In the current architecture (Stages 1–4), persistence of user data is deferred (no database) – meaning this feature is initially designed for stateless auth. For example, a magic link JWT might encode the user’s email and an expiration, without needing a user record in a database. This feature ensures the app can be used in a secure, multi-user environment and is especially relevant if the service is deployed internally or commercially. *(Note: Authentication is built in the backend and enforced in the API, but the frontend UI for login may be minimal in initial stages due to the lack of a database – the focus is on enabling the core generation features first.)*

## User Experience

The user experience of Joshua PowerPoint is centred around an intuitive chat-based interaction that guides the user from initial idea to final presentation. The design emphasises simplicity, real-time feedback, and ease of use, even for non-technical users. Below is an outline of the expected user journey and key UI/UX elements:

* **User Personas & Use Case:** The primary persona is a **time-pressed knowledge worker or educator** who needs to prepare a presentation quickly. For example, a product manager might need slides for a meeting on short notice, or a teacher might want an outline for a lesson presentation. These users may have a general idea of what they want to present but lack the hours to design slides. Joshua PowerPoint serves this persona by acting as a smart assistant that can take a brief (prompt) and do the heavy lifting of creating a draft presentation.

* **Primary User Flow – “Prompt to PPTX”:** The main flow can be described as follows:

  1. **Enter Prompt:** The user is presented with a clean chat interface prompting them to “Describe your presentation topic or objective”. They type a prompt – for example, *“Create a 5-slide presentation about the benefits of renewable energy”* – and hit send. They may also choose an AI model or adjust any available settings (such as number of slides, tone, etc.) using the model selector or controls provided.
  2. **AI Generates Outline:** The assistant (AI) processes the prompt. In the UI, a typing indicator or loading spinner may be shown to indicate activity. Within a couple of seconds, the chat interface displays the AI’s response: a structured outline of the presentation. For instance, the chat might now show a message from the AI listing 5 slide titles with bullet points under each – this is the **SlidePlan** returned from the backend. The outline appears as part of the chat conversation, allowing the user to scroll and review it easily. Each slide outline is clearly delineated (possibly styled as a preview card or just text list).
  3. **Iterative Refinement (optional):** The user reviews the outline. If something is missing or not to their liking, they can type a follow-up prompt – for example, *“Add a slide about solar energy specifically”* or *“Can you simplify slide 2’s text?”*. The interface supports this iterative dialogue: the user’s new prompt is sent, and the AI returns an updated outline or adjustments. The UI updates the displayed outline accordingly, either replacing the previous version or noting changes. The chat transcript (user messages and AI responses) is preserved in the sidebar or main panel so the user can see the history of changes. This **iterative refinement loop** continues until the user is satisfied with the outline. The design of the chat ensures that it feels conversational and interactive, with each exchange building on the last (the system maintains context via the sessionId for the conversation). This approach leverages the “chat” paradigm to fine-tune the content in a natural way.
  4. **Generate Slides:** Once happy with the outline, the user initiates the slide generation. There is a prominent **“Generate Slides”** or “Build Presentation” button in the UI (likely fixed in the interface when an outline is present). The user clicks this to confirm creation of the PowerPoint file. At this point, the backend gets to work on creating the PPTX (images + assembly). The UI transitions to a progress view – for example, a status message *“Generating presentation, please wait…”* along with a progress bar or live status list. As the backend emits progress updates (for each slide completed), the frontend may update the progress bar or list slides as done (e.g. “Slide 1/5 complete… Slide 2/5 complete…” in real-time). This real-time feedback is achieved via the WebSocket connection and gives users assurance that things are moving. The UI is designed such that the user cannot inadvertently trigger multiple jobs or navigate away easily – perhaps a modal or disabled state covers the app during generation.
  5. **Download PPTX:** When generation is finished, the user is alerted that the presentation is ready. For instance, the progress indicator reaches 100%, and a **“Download Presentation”** button becomes active. The app may also automatically initiate the download of the file, depending on settings. The download is handled through a secure link, and the file starts downloading via the browser. The UI may also present a link or a prompt like “Your presentation is ready! Click here if the download didn’t start.” The **download manager** ensures even if the file is large or the connection is slow, the download can resume and complete reliably. At this stage, the user has the PowerPoint file saved locally and can open it in Microsoft PowerPoint or compatible software. They can then review, edit, or present it as needed.
  6. **Further Editing (outside app):** While outside the core scope of the app’s functionality, it is expected that the user might fine-tune the downloaded slides in PowerPoint directly (e.g. adjusting design or adding speaker notes). Joshua PowerPoint’s goal is to get them *80-90% of the way there* with a solid first draft of slides.

* **Secondary Flows & UI Considerations:**

  * *Starting New Session:* The app likely allows starting a new presentation session (chat) via a **New Presentation** action. Given a sidebar layout (the app includes a Sidebar component in its layout), the sidebar might list past or current sessions by title, or provide options (like “New +”). With no persistent database in early stages, session management might be ephemeral (lost on refresh), but the UI is structured to handle multiple sessions if needed (the routing includes a dynamic chat session route `chat/$sessionId`). For now, users can simply refresh to start over, or the app automatically handles a single session.
  * *Model and Settings:* The presence of a *ModelSelector* in the UI components suggests users can pick which AI model to use (for example, a faster model vs a more advanced one). The UI likely offers this selection at the start or as a setting icon. Additionally, there could be an option to specify number of slides, or language/tone of presentation as part of the prompt or via toggles.
  * *Error Handling UI:* If something goes wrong (e.g., the LLM fails to generate output or the image service is down), the UI will inform the user. Error strategies include toast notifications for transient errors (with a retry option) and inline error messages for validation issues. For instance, if the user’s prompt is too short or missing required info, the input field could be highlighted with a message (client-side validation). If the backend returns an error (500, etc.), a user-friendly message like “Sorry, something went wrong. Please try again.” is shown. During generation, if the WebSocket disconnects, the app will attempt to reconnect with exponential back-off and notify the user if generation progress can’t be updated in real-time.
  * *Responsiveness:* The frontend is built as a modern single-page application with a responsive design. The UI should work on common modern browsers (Chrome, Firefox, Safari, Edge) and adapt to different screen sizes. For instance, on a desktop, the sidebar and main chat window are visible side by side, whereas on a mobile device, the sidebar might collapse into a menu. The technology stack (React + Tailwind CSS) facilitates building a mobile-friendly interface. Testing includes cross-browser verification and Lighthouse performance audits to ensure the app is fast and accessible on various devices.
  * *Visual Design:* The application uses a clean, minimal design system, likely based on **shadcn/UI** components and Tailwind CSS utility classes. This gives it a modern look-and-feel consistent with current design trends (flat design, proper spacing, accessible colour contrast, etc.). Subtle animations (via Framer Motion) are used to enhance UX – e.g., slide previews might fade in, or chat messages might animate on appearance for a polished feel. The layout includes a persistent header and sidebar (for navigation or branding) and a main content area for the chat and results, as outlined in the component structure.
  * *Accessibility:* Strong emphasis is placed on accessibility compliance (WCAG 2.1 AA). The UI is built with semantic HTML and proper ARIA roles for screen reader compatibility. Keyboard navigation is supported (users can tab through the chat input, send button, etc., and use Enter to send). Colour schemes meet contrast requirements. The development process includes automated accessibility testing (e.g., using `cypress-axe` for Axe-core checks during end-to-end tests). This ensures the app can be used by people with disabilities and meets enterprise standards for inclusivity.
  * *Performance & Feedback:* The app is optimised for fast interaction – code-splitting and lazy loading are used to keep initial load small, and heavy computation is offloaded to the backend. Frontend uses optimistic UI updates and loading spinners where appropriate to keep the experience smooth. A Lighthouse performance score of 90+ is targeted, meaning users should see a quick load and snappy interface even on mobile networks. Observability in the UI (through console logs or devtools) is available for developers, but in production the focus is on providing clear feedback to the end-user at each step of the process (as described in the flow above).

Overall, the user experience is designed to feel like **chatting with an AI assistant that builds your presentation for you**. The interface guides the user from input to result in a conversational manner, while abstracting away technical complexity. By following familiar patterns (chat UI, progress bars, download links) and ensuring reliability through thoughtful UX (error handling, real-time updates, accessible design), Joshua PowerPoint aims to make AI-powered presentation generation both powerful and pleasurable for the end user.

## Technical Architecture

Joshua PowerPoint is built with a modern web stack, organised in a **monorepo** that contains both the frontend React application and the backend FastAPI service, along with shared configuration and infrastructure code. The architecture is designed for type-safety, scalability, and maintainability, adhering to best practices at each layer. Key aspects of the technical architecture include:

### Frontend Architecture (React 18 + TypeScript)

The frontend is a **single-page application (SPA)** built with React 18 and TypeScript, bootstrapped by the Vite bundler (v5) for fast development and hot-module reloading. The code is structured into a modular component hierarchy and uses a modern state management and routing approach:

* **Component Structure:** The source tree (in `frontend/src`) is organised by feature and reuse. For example, `components/chat/` contains chat-related UI components (chat container, message bubble, input box, etc.), `components/slides/` contains slide preview/generator components, and `components/layout/` has shared layout elements (header, sidebar, etc.). This separation ensures a clean concern split between chat UI and slide display/generation UI. Shared or base UI components (like buttons, inputs, cards) are provided by a library of pre-built components (shadcn/UI) which are customised via Tailwind CSS for consistent styling.
* **State Management:** The app uses **Zustand v4.5** for client-side state management. Different slices of state are defined in dedicated stores (e.g., `ChatStore`, `SlideStore`, etc.), enabling components to subscribe only to the data they need. Zustand (with the immer middleware for immutable state updates) allows for a simple but powerful global state, without the complexity of Redux. For example, the chat store might hold the current session’s messages and slide plan, and the slide store might hold the state of a generation job (progress, completed file URL, etc.). The architecture avoids over-fetching by using **TanStack Query** (React Query v5) for server state – all API calls (like generating outline or building slides) are wrapped in query/mutation hooks that handle caching and loading states. This combination of Zustand for client state and TanStack Query for server interactions yields an efficient data flow: local UI state and remote data are managed distinctly but seamlessly.
* **Routing:** Routing is handled by **TanStack Router v1**, configured manually (not using older React Router or any Vite plugin). The application defines routes for the main sections: e.g., a root route (`/`) which might redirect to a new chat session, a chat route (`/chat/:sessionId`) for the interactive chat interface, and possibly a slides route (`/slides/:slideId`) if viewing past generated slides was supported. The router setup uses code-splitting to lazy-load route components, improving performance. Routes are composed in a type-safe manner, ensuring compile-time checks on navigation.
* **API Client Integration:** The frontend does not directly call fetch on endpoints; instead, it uses **auto-generated TypeScript API clients** derived from the backend’s OpenAPI schema. A tool (`openapi-typescript-codegen`) is run (especially after backend changes) to produce `api.ts` definitions and strongly-typed hooks for each endpoint. This means when the frontend calls `generateChat` or `buildSlides`, it does so through functions that have exact request/response types matching the backend. This significantly reduces integration bugs. These functions are integrated with React Query: e.g., a `useGenerateChatMutation` hook wraps a POST `/chat/generate` call, and a `useBuildSlidesMutation` wraps the POST `/slides/build` call, managing loading and error states automatically. The OpenAPI-driven approach ensures **contract fidelity** between front and back – if the backend changes an endpoint or data model, the TypeScript types update, catching mismatches at compile time.
* **Real-Time Communication:** For the WebSocket functionality, the frontend uses the Socket.IO client (v4) library. A custom hook `useWebSocket(sessionId)` encapsulates the connection logic. When invoked, this hook creates a Socket.IO client instance pointing at the backend’s WebSocket endpoint (for instance, `ws://<server>/ws` or similar, defined by `VITE_WS_URL` env) and attempts a connection, including authentication data (the `sessionId` as part of the connection auth payload). The hook ensures the socket is connected when the component mounts and disconnected on unmount, preventing leaks. Components can use this hook to subscribe to events like `slide:progress` and `slide:completed` – for example, the SlideGenerator component might attach listeners to update state when these events arrive. This abstraction makes real-time updates straightforward in React and maintains type safety by defining event payload shapes (e.g., `{slideIndex: number}` for progress).
* **Styling & UI/UX:** The interface is styled using **Tailwind CSS** (configured with a custom theme). This allows rapid UI development with utility classes and enforces consistency. Complex interactive components (dialogs, menus) leverage Headless UI or Radix under the hood via shadcn’s library, providing accessible patterns out-of-the-box. Framer Motion is used for animations, for example animating message appearance or slide previews, adding polish to the UI.
* **Build and Tooling:** Development uses Node.js 18 and a suite of tools: ESLint (with Typescript and React plugins) for linting, Prettier for formatting, Vitest + React Testing Library for unit tests (with a target of ≥ 90% coverage on UI code), and Cypress for end-to-end tests. All these tools are configured to run via **Docker** as well as locally, to ensure consistency (for example, `npm run test` actually triggers tests inside a Docker container that mimics CI). The frontend is containerised with a Dockerfile, and both development and production builds run in Docker (with multi-stage builds to produce a lean Nginx-served image for deployment). This consistent environment avoids “works on my machine” issues, as all contributors and CI use the same Node version and dependencies as pinned in `versions.md`.
* **Performance & Optimisation:** The app employs code splitting (each major route or component chunk is split out) and tree-shaking via the Vite/Rollup bundler to keep bundle size small. Only modern evergreen browsers are targeted, allowing use of latest JS features and reducing polyfill bloat. TanStack Query caching is tuned (e.g., slide outline results might be cached for quick re-display if user navigates away and back) and images are optimised (converted to WebP and lazy-loaded). All frontend code passes through a CI pipeline enforcing performance budgets (Lighthouse CI ensuring, for instance, < 250 KB total JS, Time-to-Interactive < 3s on mobile). These measures ensure the frontend remains fast and responsive, complementing the efficiency gains provided by the backend.

### Backend Architecture (Python FastAPI)

The backend is a **FastAPI (Python 3.12) application** that exposes a RESTful API and manages the AI integration logic, slide generation, and other server-side concerns. It’s built for high performance (using Uvicorn ASGI server) and strict type safety through Pydantic models. Key elements of the backend design include:

* **Service-Oriented Modules:** The backend code is organised under `backend/app/` with clear modules for different services: e.g., `api/` for API route definitions, `models/` for data models (Pydantic schemas), `services/` for business logic integration with external APIs (LLM, image generation, PPTX creation). This separation aligns with the core features:

  * *LLM Service (`services/llm.py`):* Handles communication with the LLM provider (OpenRouter API) to get slide outlines. It sends the user’s prompt and parameters to the OpenRouter’s chat completion endpoint and receives a response which it validates and transforms into `SlidePlan` objects. It ensures consistent camelCase keys and proper error handling (e.g., if OpenRouter returns an error or invalid JSON, it raises exceptions that translate to an HTTP 502 or 500 for the client).
  * *Image Service (`services/images.py`):* Connects to the Runware API to generate or fetch images for slides. Given a list of `SlidePlan` (which might include textual cues for images), it calls the image generation API for each plan or in batch and collects image metadata (URL or local file paths). This service also uses retries on failures and can operate asynchronously (using `httpx.AsyncClient`) to perform parallel API calls for efficiency.
  * *PPTX Builder (`services/pptx.py`):* Takes the slide outline and images and composes a PowerPoint file. It uses the `python-pptx` library to create a new presentation, then iterates through slides adding content: sets slide title, adds bullet points in the body text box, and inserts the image (if available) in a content placeholder or a predefined position. Finally, it saves the file to a temporary location and returns the path. This module abstracts the file creation details from the rest of the app.
  * *Auth Module:* Manages user authentication. FastAPI dependency functions handle verifying JWTs on protected endpoints. A utility to send magic link emails would be included (in Stage 2, this might simply log a link or use a service like AWS SES in the future). Passwordless login is implemented by generating a signed token and including it in an email URL; clicking it calls back to the app to exchange it for a session JWT. The JWT signing key and settings are configured via environment (and kept secure in deployment via secrets).
  * *WebSocket Endpoint:* The backend includes an ASGI app for Socket.IO. This is mounted at a route (e.g. `/ws` or integrated via `socketio_app.py`). The Socket.IO server (python-socketio) is initialised with event handlers for `connect` (to authenticate the session, perhaps using sessionId or JWT) and to emit progress events. On the server side, when a slide generation job starts, the backend might spawn a background task and immediately return a job ID to the client, then that background task will emit events on the Socket.IO server as it progresses.
* **API Endpoints & Schema:** The REST API surface is well-defined and documented via OpenAPI 3.1. Key endpoints include:

  * `POST /chat/generate` – accepts a JSON payload (matching the `ChatRequest` model: prompt, slideCount, model, etc.) and returns a `ChatResponse` containing the generated slide outline (list of `SlidePlan` objects) and perhaps a conversation/session identifier. This is synchronous and typically quick, as it just involves the LLM call.
  * `POST /slides/build` – initiates the slide deck generation process. This might return immediately with a job identifier or a simple acknowledgement that generation started. The heavy lifting (calling image service and PPTX builder) is done asynchronously (potentially via a Celery worker for scalability). The client is expected to listen on the WebSocket for `slide:completed` event rather than getting the PPTX in this response. In some implementations, the response could also include a presigned download URL if generation was done synchronously, but in our design, it’s likely async for anything beyond trivial cases.
  * `GET /slides/download?jobId=XYZ` – (If implemented) a protected endpoint to download the PPTX file corresponding to a completed job. In practice, the design uses the presigned URL approach via WebSocket, so this may not exist; instead, a static file service or S3 link is used. However, a health-check or status endpoint could be present to query job completion if WebSocket is not used.
  * `/auth/login` and `/auth/refresh` – for authentication flows as described. `/auth/login` might accept an email and send a link, and `/auth/refresh` accepts a refresh token and returns a new JWT pair (access + refresh).
  * `GET /health` – a simple health check endpoint for monitoring (returns 200 OK if the service is up, possibly with version info or uptime).
    All endpoints have rigorously defined request and response models using Pydantic, which feed into the OpenAPI documentation. The OpenAPI schema is the single source of truth for contracts between frontend and backend. This schema is frozen at certain points (e.g., before integration testing in Stage 4) to ensure front and back are in sync.
* **Data Models:** The backend uses **Pydantic v2** for data modelling and validation. Key models include:

  * `ChatRequest` / `ChatResponse` – define the shape of the prompt input and AI output respectively.
  * `SlidePlan` – represents a slide outline (e.g., fields: `title: str`, `bulletPoints: list[str]`, maybe `notes: str`). This model is used both server-side and is also mirrored in the TypeScript types for the frontend.
  * `ImageMeta` – holds info about generated images (e.g., `url: HttpUrl` or `path: str`, plus maybe metadata like alt text).
  * `PPTXJob` – might represent the status of a generation job (with fields like jobId, status, progress, error message, etc.), if such tracking is needed outside of WebSockets.
    These models enforce correctness – for example, Pydantic will validate that slide titles are strings of a certain length, or that image URLs are properly formed. All models use **strict mode** (ensuring no implicit type coercion) to catch issues early. They also define JSON serialization (using camelCase keys to align with frontend expectations). This means the data sent over the wire is directly usable by the frontend’s conventions.
* **Business Logic & Flow:** When a user triggers the generation of slides:

  1. The `POST /chat/generate` handler is invoked. It creates a `ChatRequest` object from the JSON payload and calls `llm_service.generate_outline(request)` (for example). The LLM service contacts OpenRouter (with the appropriate API key from config) and gets a draft outline. The result, a list of slide plans, is returned in the HTTP response as JSON. This call is relatively quick (dependent on the LLM response time, typically a couple seconds).
  2. When the user confirms generation (`POST /slides/build`), the API handler will do a few things: authenticate the user (if required for this action), possibly record an entry (e.g. log an audit or an entry in an in-memory job map), then initiate the process of building slides. In a simple implementation, it might call the image service and PPTX builder sequentially then return the file, but to improve UX (not blocking the HTTP call), it likely spawns a background task (using FastAPI’s `BackgroundTasks` or an async queue). For scaling and reliability, the architecture by Stage 5 incorporates **Celery with a Redis broker** to handle this job asynchronously. The FastAPI app would enqueue a “create\_pptx” task with the current slide outline data; Celery worker processes the task, and meanwhile the API returns immediately (with a 202 Accepted and job ID, or just a message “generation started”).
  3. The Celery worker executes the task: calls `generate_images(slides)` (which in turn calls the Runware API, possibly concurrently for all slides). Once images are ready (or placeholders set for any failures), it calls `build_pptx(slides, images)`. During this process, the worker can emit WebSocket events via the Socket.IO server: after each slide image is fetched, emit `slide:progress` (with slide index updated). Finally, when `build_pptx` returns the file path, the worker could upload the file to S3 (if not already on a shared volume) and obtain a presigned URL. It then emits a `slide:completed` event with that URL for download. If any exception occurs during generation, an error event could be emitted to inform the user.
  4. The user’s browser, upon receiving the `completed` event, triggers the download. The actual download is handled by the front’s Download Manager which makes an HTTP GET to the provided URL (which might be an S3 bucket URL or a `/slides/download?jobId` API that streams the file).
* **Performance & Concurrency:** FastAPI is asynchronous, and the design takes advantage of that. The LLM and image calls are I/O bound, so they are `async` functions that do not block the event loop. For example, the image service can fire off multiple HTTP requests concurrently using `httpx.AsyncClient` with `asyncio.gather` for improved speed. The use of Celery (with potentially multiple worker processes) means heavy CPU-bound tasks (like the Python-pptx file creation if it involves image encoding or large file writing) can be offloaded and parallelised across cores or machines, keeping the main API responsive.
  Observability hooks like Prometheus metrics are integrated – e.g., the backend records metrics such as request latency, number of slides generated, etc. A `/metrics` endpoint (for Prometheus scraping) is exposed to track performance in real scenarios. Logging is structured (using `structlog` to emit JSON logs) so that each request and job can be traced with an ID, and any errors are logged with stack traces for debugging.
* **Security:** The backend enforces CORS (allowing only the known frontend origin to call the APIs) and rate limiting on endpoints to prevent abuse – for instance, a limit of 100 requests per 15 minutes per IP on generation endpoints is configured via a middleware. All secrets (OpenRouter API key, Runware API key, JWT signing key) are managed via environment variables and (in production) injected through a secrets manager (e.g., sealed secrets in Kubernetes or Vault). The JWT tokens for auth are signed with HS256 and have short lifetimes (15 minute access token, 24 hour refresh) to mitigate risk if stolen. Furthermore, the codebase is scanned with security linters (Bandit for Python) as part of CI and dependency scanning (e.g., using `pip-audit` or Docker image scans via Trivy) to catch vulnerabilities.

Overall, the backend is crafted to be *contract-first* (define the API and models, then implement), ensuring that by the time integration happens, the frontend and backend speak the same language. It also strives for resilience – with retries for external calls, fallbacks for missing data, and clear logging – so that the user experience remains smooth even if underlying services are flaky or slow. By Stage 2 completion, the backend delivers a fully functional API that meets the product requirements (with >90% test coverage and no high-severity security issues), forming the solid engine behind the Joshua PowerPoint application.

### Infrastructure & DevOps

The deployment architecture and development operations for Joshua PowerPoint adhere to modern cloud-native practices, ensuring the application can be built, tested, and released reliably. Key points include:

* **Containerisation:** Both frontend and backend are containerised with Docker. Multi-stage Dockerfiles produce lean images (for example, the production backend image is ≤250 MB in size after using a builder stage and running as non-root). Docker Compose configurations are used for local development to run the entire stack (frontend, backend, and supporting services like a fake SMTP or a Redis if needed). This ensures that the development and test environments mimic production closely. The Stage 1 setup established that *all developers must run code and tests in Docker to avoid environment drift* – e.g., Node and Python versions are pinned and enforced in the Docker images.
* **Continuous Integration (CI):** A GitHub Actions CI pipeline is configured to run on every pull request and push to main. The CI workflow, defined in `.github/workflows/ci.yml`, includes stages for install, type-checking, linting, testing, contract testing, performance tests, build, and security scans. For instance, it will:

  * Install dependencies (npm and pip) in a fresh environment.
  * Run TypeScript type checks and Python type checks (mypy).
  * Lint the code (ESLint, Stylelint for frontend; Ruff and Black for backend).
  * Run all unit tests (frontend Vitest, backend Pytest with coverage), aiming for the required coverage (≥ 90%).
  * Perform **contract testing** using Schemathesis or Pact: the backend’s OpenAPI is tested against expectations, and the frontend’s API calls are validated against the backend (in Stage 4, consumer-driven contract tests were set up to ensure frontend and backend remain in agreement on the API).
  * Run performance tests in CI (using k6 for a smoke test to ensure that a basic load of e.g. 10 virtual users passes within thresholds).
  * Build Docker images for frontend and backend and scan them for vulnerabilities (using Trivy, ensuring no critical CVEs).
  * Possibly run end-to-end tests in CI: for example, spin up the backend and a headless frontend (or use the built frontend bundle) and run Cypress tests that simulate the full user flow (with the option to stub external APIs).
    Only if all steps pass does the pipeline allow merging or deployment. This rigorous CI catches issues early and guarantees that main branch is always in a deployable state.
* **Continuous Delivery & Deployment:** The project follows a phased release strategy. Merges to the `main` branch trigger automatic deployment to a **staging** environment (using a lightweight Kubernetes cluster, e.g. k3s). A separate workflow (`deploy.yml`) handles this: after CI build is successful, it pulls the latest images, signs them (Sigstore Cosign is used for container signing), and then deploys via Helm charts to the cluster. The Helm chart defines the Kubernetes resources: a Deployment for the backend (FastAPI Uvicorn workers), a Deployment for the frontend (served via Nginx), a Service and Ingress (Traefik) for routing, a Redis instance (for Celery and caching), etc. Blue/green deployment is implemented to release new versions with zero downtime – essentially the Helm upgrade is configured to deploy a new set of pods and then switch traffic once they’re healthy. For production, a manual promotion is required: either by pushing a git tag or using a `release/*` branch which triggers a deployment to production with canary analysis (using Flagger for automated rollback if errors exceed 1%). This ensures that any production release has been through staging and meets reliability criteria.
* **Infrastructure Components:** In production deployment, aside from the app containers, the architecture includes:

  * **Kubernetes (k3s):** Chosen for a lightweight yet production-capable k8s; it hosts the application pods. Traefik ingress provides routing and TLS termination.
  * **Redis:** Used as a **cache and message broker**. The backend may cache certain results (though currently stateless, Redis can be used to store e.g. rate-limit counters or ephemeral session data). More critically, Redis serves as the Celery broker and possibly result backend, enabling the asynchronous task queue for slide generation. This decouples immediate API responses from long-running tasks.
  * **Object Storage (S3 or equivalent):** Used to store generated PPTX files and any other large artefacts (perhaps images, though those might come from external service directly). When a deck is generated, it’s uploaded to S3 and only a link is sent to the client, rather than sending a potentially tens-of-megabytes file through the API. This is scalable and offloads bandwidth from the app server. Backups of important data (if any, e.g. logs or future database) are also stored in S3.
  * **Monitoring & Observability:** A full **observability stack** is deployed. Prometheus monitors metrics from the app (via `/metrics` on the backend, which includes custom metrics like API latency, slides generated count, etc. and default ones). Loki aggregates logs from the pods for easy querying of application logs. Grafana is used to visualise both metrics and logs; dashboards are set up for overall system health, performance (latency histograms, throughput), and slide generation specifics (like distribution of generation time). Additionally, Tempo (or OpenTelemetry tracing) is configured to trace requests across services – for example, a single user prompt might be traced from the API call to all subsequent service calls (LLM, image, Celery task) and back, helping pinpoint bottlenecks. Alerts are configured via Alertmanager to page the on-call team if SLOs are violated (e.g., if P95 latency goes above 400ms for a sustained period, or error rate spikes above 2%).
  * **Security & Secrets:** Sensitive configuration (API keys, JWT secret, etc.) is managed through Kubernetes secrets or Vault. For staging, sealed-secrets (kubeseal) might be used to encrypt secrets in Git, whereas production uses Vault with tighter controls. Additionally, supply chain security is considered: container images are signed (Cosign) and verified before deployment, and an SBOM (Software Bill of Materials) is generated to track dependencies and known vulnerabilities (Trivy’s scan output is uploaded to GitHub’s security tab). Periodic dependency updates are done to keep libraries up-to-date.
* **Testing & Quality Gates in Pipeline:** Before deployment to production (Stage 5), the system undergoes rigorous testing in an integrated environment (Stage 4). This includes:

  * **Contract Testing:** Using Pact, the frontend (consumer) and backend (provider) APIs are tested against each other to ensure no integration drift. Pacts are published and verified in CI.
  * **End-to-End Testing:** Comprehensive Cypress tests run against the staging environment (or a ephemeral environment created in CI) to simulate a user’s journey *prompt → outline → generate → download*. The E2E tests verify that a user in Chrome or Firefox can successfully obtain a PPTX from a given prompt with no errors. They also check things like the file is not corrupted (by validating the PPTX file structure or simply its download checksum).
  * **Performance Testing:** The app is stress-tested using k6 scripts. Load tests ramp up to 150 concurrent users generating slides, verifying the system meets performance budgets (e.g., P95 request duration under 400ms for outline generation, and the system can handle sustained load without breaking). Results are used to tune resources or code if needed.
  * **Security Testing:** Automated scans such as OWASP ZAP run against the staging deployment to catch any common web vulnerabilities (XSS, SQL injection (not likely since no SQL yet), insecure headers, etc.). Any high or critical findings must be resolved before release. The infrastructure is also tested – e.g., ensuring no sensitive ports are open, and that their disaster recovery plan (backups, restore) meets the ≤15min RTO objective.

In summary, the technical architecture of Joshua PowerPoint spans a **React 18 frontend** and a **FastAPI backend**, connected by well-defined APIs and real-time channels, all running in a containerised environment. The stack leverages modern frameworks (TanStack, Pydantic, Socket.IO) to achieve type safety and reactivity, and employs robust DevOps practices (CI/CD, monitoring, security scans) to ensure the product can be delivered with confidence. The design intentionally avoided adding a database or complex persistence in the initial phases to speed up development and focus on the core slide generation functionality – simplifying the system to mostly stateless services. This choice, along with a strict adherence to pinned versions and cross-platform consistency (developing *in Docker* is mandatory), has resulted in a reliable foundation that can be incrementally built upon in each stage of the project.


## Logical Dependency Chain

The development plan for Joshua PowerPoint is structured to tackle foundational elements first and then layer on functionality, enabling the team to achieve a working end-to-end product as early as possible and iteratively enhance it. Here is the logical sequence of development and how each part depends on previous work:

1. **Foundation First:** Stage 1 (Foundation) is the prerequisite for everything – it sets up the development ecosystem. Without Dockerisation, CI, and a baseline project structure, development in later stages would be inconsistent and error-prone. By enforcing tooling and conventions upfront (like pinned versions of Node/Python and uniform dev environments), we mitigate cross-platform issues early. Thus, Stage 1 must be completed (repository structure and CI in place) before any serious backend or frontend coding begins. It’s the platform on which all other stages build.

2. **Backend API & Logic before Full Frontend Integration:** Stage 2 (Backend) and Stage 3 (Frontend) are somewhat parallel but with critical interdependencies:

   * *API Contract as the Bridge:* Early in Stage 2, the product team and developers define the API contract (OpenAPI draft) for the core features. This allows Stage 3 frontend developers to start work using mock responses shaped according to this contract, even as the backend implementation is underway. The **logical dependency** is that the frontend needs a stable interface to interact with – thus, the backend’s API spec (if not the full implementation) should be agreed upon first. In practice, Stage 2 produces an OpenAPI spec v0.9 that is used by Stage 3 to generate type-safe clients and hooks.
   * *Incremental Integration:* While Stage 2 is building out the actual logic (LLM integration, etc.), Stage 3 can proceed to build UI components. The critical path is to get to a point where the frontend can call the backend and get a meaningful response. Therefore, the **chat generation** endpoint was prioritised in backend development (Task 2.6 depends on 2.1 which is domain models), because the chat outline generation is needed to power the primary UI flow. Meanwhile, the frontend’s critical tasks were to scaffold the app and implement the chat UI (Task 3-8, 3-9 depended on base layout). By completing the basic chat endpoint in Stage 2 and the chat UI in Stage 3, we achieve a **vertical slice**: a user can input a prompt in the UI and get an outline from the real backend. This was planned to happen as early as possible (by end of Stage 3) – even if the PPTX generation wasn’t fully integrated yet, having a visible result (outline) for a given prompt is a big milestone.
   * *Frontend before Backend completion?* While ideally the backend core features finish before frontend integration, some parts of Stage 2 (like authentication or some error handling) are not blockers for the main user flow. Thus, Stage 3 could start once the outline generation API is roughly in place and the team has confidence in the endpoints’ shape. In parallel, the backend could still be developing slide generation and other internals. The chain of dependency was managed such that by the time Stage 3 needed to integrate the “Generate Slides” action, the backend’s `/slides/build` was ready to be used. If not, Stage 3 employed **mocks/stubs** for that piece, allowing frontend work to continue without delay. This flexibility ensured that the lack of a fully finished backend wouldn’t stall UI development – the two tracks converged in Stage 4 when everything was plugged together.

3. **Prioritising a Usable Frontend Early:** A guiding principle was to get “something visible and usable” as soon as possible. That meant:

   * After Stage 2 and 3, by the beginning of Stage 4, we expected to have a basic end-to-end path working (perhaps with some manual steps or mocks). Indeed, by end of Stage 3 we had a local demonstration of *prompt → (mocked) PPTX download*. This early integration (even if partial) was important to validate the concept and gather feedback.
   * The **critical path** through tasks was designed to support this. For example, in the frontend WBS, the critical path was: scaffold app → setup routing → build core layout → implement chat UI → wire optimistic UI → then E2E test. Non-critical features (like Storybook or certain optimisations) were done later or off the critical path. This ensured that the core functionality was up and running first, with enhancements following.
   * Similarly on backend, critical tasks (domain models, LLM service, image service, PPTX builder, API routes) were done early (Tasks 2.1 through 2.6) before less critical ones (like observability hooks or extra testing harness). This sequencing means the core logic was ready to be called by Stage 3, and things like detailed observability (Task 2.9) could even be added afterwards without blocking the frontend.

4. **Building Iteratively & Atomically:** Each feature or component was built in a way that it could function on its own, but also serve as a foundation for the next:

   * For instance, the **Chat UI** was built to display outlines as soon as the data was available, even if initially that data was static. Once the real API was connected, no major rewrite was needed – it was already prepared to handle dynamic data.
   * The **LLM integration** was built and tested in isolation (returning slide plans from prompts) so that by the time the frontend connected, it “just worked.” Then the image generation was added, then the PPTX builder – each could be tested via the API or unit tests independently. When chained together, they provided full functionality.
   * The architecture deferred certain features (like persistent storage or multi-user management) precisely to keep the dependency chain short and focused. By avoiding a database in early stages, we removed an entire category of dependencies (migrations, data models, etc.) that could slow down delivering the core value. This will be revisited once the core is stable.
   * Stage by stage, we also ensured that **quality gates** (tests, linting) were not compromised. This is part of logical dependency too – e.g., one cannot integrate (Stage 4) properly if Stage 2 and 3 didn’t have tests (one might break something without noticing). So a rule was enforced: each stage only “exits” when its quality criteria are met (test coverage, etc.), which in turn ensures the next stage isn’t fighting fires from the previous one.

5. **Rapid Feedback and Adjustments:** By getting a working prototype early (end of Stage 3/beginning Stage 4), we allowed for iterative improvements. For example, if the AI’s slide outline wasn’t formatted nicely in the UI, the team could adjust the presentation of bullet points immediately. Or if performance of image generation was a concern, they could consider in Stage 4 whether to add caching or limit image resolution. This agile approach – deliver a thin vertical slice, then widen it – was facilitated by the logical ordering of tasks.

In summary, the development followed a **logical dependency chain where foundational work (Stage 1) enabled backend API development (Stage 2), which in turn (through a defined contract) enabled frontend development (Stage 3).** These came together for integration testing (Stage 4), all of which is required before a production deployment (Stage 5). The focus on “working software asap” meant that by respecting these dependencies, we had a usable front end connected to at least a partially functional backend by mid-cycle, rather than building all backend then all frontend sequentially. This reduces risk and helped ensure that by the time we invest in polish and deployment, we know the core product works. Each stage’s outputs feed directly as inputs to the next – for example, **Stage 2’s OpenAPI spec and Dockerised service are inputs to Stage 3 and 4**, Stage 3’s UI and tests are inputs to Stage 4’s integration, and Stage 4’s validated package (containers + charts) is the input to Stage 5 deployment. By following this chain, we achieved a coherent and timely development cycle.

## Risks and Mitigations

Developing and deploying Joshua PowerPoint involves several technical and strategic risks. Here we identify key risks and how we plan to mitigate them:

* **Performance Bottlenecks:** The process of generating a full presentation (especially with external API calls for AI and images, and on-the-fly PPTX assembly) could be slow or resource-intensive. If not managed, users might face long waits or timeouts. *Mitigations:* We have set clear performance targets (e.g., slide generation ≤ 30s for a 5-slide deck, API latency P95 ≤ 400ms) and will test against them with tools like k6 under realistic load. The architecture uses asynchronous processing and can be scaled horizontally – e.g., multiple Celery workers for concurrent generation, and the system is stateless so more instances can be added behind a load balancer. We also built in caching layers (e.g., if the same image is requested or same prompt given in a short time, we could reuse results) and efficient resource usage (streaming file writes, etc.). Monitoring in production will catch performance drifts, and alerts (e.g., if P95 latency > 450ms) will prompt investigation. In the worst case, we can impose limits (like maximum slides allowed) to control load, and optimise bottlenecks in code (using faster libraries or caching results from the LLM if similar requests happen).

* **Cross-Platform and Environment Inconsistencies:** Given the mix of technologies (Node, Python, Docker, different OS for dev vs prod), there’s a risk of “it works on my machine” issues or differences between environments causing failures. *Mitigations:* Right from Stage 1, we enforced Docker for all dev and CI tasks. This means whether a developer is on Windows, Mac, or Linux, they run code in a uniform container environment where dependencies are the exact same versions. All tools are pinned in `versions.md` (Node 18.19.0, Python 3.12.0, etc.), preventing version drift. The CI pipeline runs all tests inside containers, mirroring production conditions, so any platform-specific issue is caught early. We also provided troubleshooting docs (for example, reminding to delete `node_modules` if switching host OS to avoid native module mismatches). This strict containerisation and documentation has already proven effective during development to maintain consistency across platforms.

* **Integration Mismatch (Frontend-Backend Contract):** A classic risk in such projects is that the frontend and backend, developed somewhat in parallel, might not perfectly align (e.g., field names, error codes). *Mitigations:* We adopted a **contract-first development** approach. The OpenAPI schema is the single source of truth and we use codegen to ensure the frontend knows exactly what to send/expect. Additionally, we implemented contract testing (Pact/Schemathesis) to continuously verify that the backend meets the frontend’s expectations. This catches any divergence quickly. During integration (Stage 4), any issues discovered were fixed on either side with tests added to prevent regression. The strong typing on both ends (TypeScript for frontend, Pydantic for backend) also makes many integration issues compile-time errors rather than runtime (e.g., if backend changes a field type, the frontend’s generated types will flag a mismatch). These practices greatly reduce the risk of integration bugs reaching production.

* **Reliability of External Services:** Joshua PowerPoint relies on external APIs (the LLM via OpenRouter and the image generation via Runware). Downtime or slowness of these could degrade our service or cause failures. *Mitigations:* We implemented robust error handling and retry logic for these calls. The LLM and image services are called with timeouts; if a call fails or times out, the system will retry up to 3 times with exponential backoff. If ultimately the service is down, our backend will catch the exception and return a graceful error message to the frontend (which then shows a user-friendly error). For image generation specifically, if it fails, we use a placeholder image so the presentation can still be built and downloaded, thereby providing partial functionality instead of total failure. In the long term, we might consider caching commonly generated images or having a fallback image service. Also, by abstracting these services in our code, we maintain the flexibility to swap providers if one becomes unreliable or too costly (as long as the alternative provides similar functionality). Monitoring includes tracking failures of external calls; if we see frequent issues, we can respond (e.g., alert the user that generation might be slow, or disable that feature temporarily).

* **Security Vulnerabilities:** As an application that deals with user input (prompts) and generates downloadable content, security is crucial. Risks include injection attacks, cross-site scripting (XSS) via the web interface, leakage of sensitive keys, or malicious files. *Mitigations:* We have taken multiple steps:

  * The backend validates and sanitises inputs with Pydantic, reducing injection risk (and since we have no SQL database at this stage, SQLi is moot). The prompt text is used only in calls to external APIs (OpenAI) and in presentation content; we ensure that when the content is inserted into the PPTX, it does not inadvertently execute anything (the PPTX is essentially data, but we avoid, say, writing out macro-enabled PPTM files).
  * The web frontend escapes or neutralises any AI-generated content when displaying it. For example, if the AI returned malicious HTML in a slide outline (unlikely but possible), we treat it as text, not HTML, thus avoiding XSS. We also use React which by default escapes content in the DOM unless explicitly instructed not to.
  * Authentication (JWT) is used to secure endpoints if needed, and all API calls are over HTTPS to prevent snooping. We store secrets (API keys, JWT signing keys) securely – e.g., in environment variables not checked into code, and in production they’re in Vault or sealed secrets. Keys are rotated on a schedule (as per policy, e.g., every 90 days for the OpenRouter API key).
  * We run automated security scans: Bandit for code security issues in Python, `npm audit` for JS packages, Trivy for Docker image vulns. In Stage 4, we ran OWASP ZAP against the deployed app to catch any web-level vulnerabilities; results showed no high/critical alerts. We also include Content Security Policy, secure cookies (if any), and other best-practice security headers in our deployment.
  * Finally, we plan for incident response: logs are kept (and monitored) for unusual activities, and if a breach or serious vuln is found, we have the ability to patch and roll out a fix quickly via our CI/CD pipeline.

* **Quality and CI/CD Pipeline Risks:** As we built an extensive CI pipeline, there’s a risk of the pipeline becoming a bottleneck (flaky tests, long build times, etc.), which could slow development or, worse, allow bugs through if not all tests are consistently reliable. *Mitigations:* We have been careful to keep tests deterministic (e.g., controlling random seeds in tests, using dedicated test containers for services). We run tests in parallel where possible (frontend and backend tests can run concurrently in CI jobs) to keep the pipeline efficient. The pipeline is also run in Docker consistent environments to reduce flakiness due to environment differences. We’ve set thresholds for test performance too – e.g., ensuring the Cypress E2E tests have a high reliability (≥95% pass rate across runs), meaning if they ever dip (become flaky), we will address the underlying cause. By strictly enforcing that all checks pass before merge, we ensure that no known issue is ignored. If the CI itself fails (e.g., a service outage in GitHub Actions), we have the option to run tests locally in Docker or use another runner as backup to not block deployments. Our use of pinned tool versions also means a sudden update in a library won’t break the build unexpectedly.

* **Scalability and Resource Constraints:** As usage grows, the workload on the system (especially the AI calls and PPT generation) could outpace resources, leading to slowdowns or crashes. This is both a technical and cost risk (AI API usage costs money). *Mitigations:* The design is cloud-scalable: stateless services behind load balancers, with a task queue that can have multiple worker replicas. We can scale out horizontally by increasing pod counts or CPU limits. We have also put in basic controls like rate limiting per user/IP to prevent abuse or accidental overload by a single user. Regarding cost, we will monitor usage of the OpenRouter and image APIs; if users start generating very large amounts, we might implement usage quotas or require confirmation for very large requests. In terms of memory/CPU, our Docker images have been optimised (non-root, slim dependencies) and we aim to keep them efficient. The observability stack will alert us if resource usage is high (e.g., if CPU regularly > 80% or out of memory events happen), so we can proactively upscale or optimise code (for instance, releasing memory after image generation, etc.).

* **Project Scope Creep / MVP Focus:** There is a strategic risk of trying to implement too many features at once (e.g., supporting full slide design customisation, adding a database for saved projects, etc.) which could delay the core delivery. *Mitigations:* We have clearly defined the MVP scope in this PRD and staged everything accordingly. Many nice-to-have features (user accounts with saved history, multi-language support, rich text formatting on slides, etc.) are consciously left out of these five stages. We note them as future enhancements (in Appendix or internal backlog) but do not let them interfere with the critical path. Stages 1-5 cover what’s absolutely needed to have a working product. By sticking to this staged plan and deferring any new ideas to after Stage 5 (or a v2.0 roadmap), we reduce the risk of scope creep. Regular review meetings will ensure we remain on track with MVP features only, and any change in requirements will be evaluated for impact on timeline and dependencies.

Each of these risks is tracked throughout the project. The mitigations are implemented as part of the development process (not as afterthoughts), which is evident in the planning documents and outcomes (for example, the emphasis on Docker and testing from day one addresses the cross-platform and integration risks head-on). By being proactive about these risks, we aim to ensure a smooth development and a robust, secure, high-quality product at launch.

## Appendix

**Technical Specifications and Additional Details:**

* **Module Specifications:** The following summarizes key modules in the system and their responsibilities (as documented in the technical design docs):

  * *Chat UI Module (Frontend):* Manages the chat-based interaction in the UI. It handles prompt input, displays the AI-generated slide outline, and supports iterative refinement of the prompt. Inputs include the `sessionId` (to identify the chat session) and the user’s prompt text; output is a list of `SlidePlan` objects shown to the user as the outline. The chat module uses React Query for sending the prompt to the backend and Zustand for storing the conversation state. It also opens a WebSocket connection to listen for live updates (like progress events). The sequence is: user enters prompt → UI calls generate API → receives outline → updates state → displays outline.
  * *LLM Service (Backend):* This is the Python service that interfaces with the LLM API (OpenRouter) to get slide outlines. It takes a `ChatRequest` (which includes the prompt, desired number of slides, and chosen model) and returns a list of `SlidePlan` Pydantic models. It handles HTTP communication with OpenRouter (POSTing the prompt and parsing the JSON response). If OpenRouter returns an error or invalid data, the LLM service will raise an HTTPException or error that the API layer catches, ensuring the client gets an appropriate error code. We ensure that the output is validated – only well-formed `SlidePlan` objects (with title and bullets as expected) are returned by this module.
  * *Image Service (Backend):* Connects to the Runware Image Generation API. The module exposes a function like `generate_images(slides: list[SlidePlan]) -> list[ImageMeta]` which goes through each slide and obtains an image. It might use keywords from the slide plan (like the title or bullet points) as input for image generation. It operates asynchronously – could launch parallel requests for speed. The `ImageMeta` contains the image URL (or a local path if downloaded). In tests, this module is mocked via `respx` to simulate image API responses. It also includes error handling: e.g., if one image fails to generate after retries, it logs the failure and could return a placeholder path for that image.
  * *PPTX Builder (Backend):* Composes the PowerPoint file. The module uses `python-pptx` to create a new presentation and add slides one by one. For each `SlidePlan`, it creates a title placeholder and a content placeholder: the title text is set, bullet points are added as paragraphs in the content box, and if an image is available, it is inserted into the slide (potentially in a predefined image placeholder or by adding a Picture shape). After processing all slides, it saves the presentation to a temporary file path and returns that path. The PPTX builder ensures that even if some images are missing, slides are still created with a default image (we have a placeholder image asset for that). This module’s correctness is key to the final output; it’s tested with sample slide data to ensure formatting is correct.
  * *WebSocket Communication:* The backend’s Socket.IO server is set up to emit `slide:progress` and `slide:completed` events during the generation process. The payload for progress includes at least a slide index or count of slides done, and for completion it includes the `pptxUrl` (which could be a full download link or an identifier). On the frontend, the `useWebSocket` hook encapsulates connecting to this Socket.IO endpoint and returns the socket object so that components can subscribe to these events easily. This real-time channel is integral for user feedback and is implemented with acknowledgement of potential disconnects (the client will auto-reconnect with backoff, and the server can replay missed messages or the client can query status if needed).
  * *Download Manager (Frontend):* Once the PPTX is ready, the frontend uses a Download Manager component/module to fetch the file. It takes the presigned URL provided and issues a GET request to download the file in the background, showing a progress bar if needed. It supports resuming by using HTTP range requests – if the download is interrupted, it can continue from where it left off by requesting the remaining bytes. If a URL has expired (perhaps the user waited too long), it can prompt the backend for a new URL (this might use the job ID to request a fresh link). The component ensures that the user is notified of any issues (network errors, etc.) and that they end up with the PPTX saved. This kind of robust download handling is especially important for potentially large files and users on slow networks.

* **Data Models (Pydantic Schemas):** Key data models and their fields:

  * `ChatRequest`: fields – `prompt: str` (the user prompt), `slide_count: int` (desired number of slides), `model: str` (AI model name or ID). It may also include a `sessionId` to tie it to a conversation (or that might be passed via headers/WS).
  * `SlidePlan`: fields – `title: str`, `bullets: list[str]`, `notes: Optional[str]`. Represents one slide’s content. This is what the LLM service returns in a list. In JSON, it’s likely an array of objects like `{ title: "Slide 1 title", bullets: ["point1", "point2"], notes: null }`. The Pydantic model uses aliases so that if the backend internally uses snake\_case, it outputs camelCase keys to match the TypeScript expectations (e.g., bullet points might be returned as `bulletPoints`).
  * `ImageMeta`: fields – `url: HttpUrl` (if image is accessible via a URL), or perhaps `path: str` (if saved locally or relative path), plus maybe `alt_text: str`. This model helps in passing image references from the image service to the PPTX builder.
  * `PPTXJob`: fields – `job_id: UUID`, `status: str` (e.g., "pending", "in\_progress", "completed", "error"), `error_message: Optional[str]`, `result_url: Optional[AnyUrl]`. This could be used if we implemented a polling mechanism or to log job status. Even if not exposed via API directly (due to using WebSockets), it’s a useful internal representation for tracking.
  * `ChatMessage` (frontend only likely): fields – `sender: "user"|"assistant"`, `text: string`. This is used in the chat UI to render past messages. It’s not a server model but maintained in frontend state or local storage if needed.

* **Project Directory Structure:** 

  ```
  repository-root/
  ├── frontend/
  │   ├── src/
  │   │   ├── components/        # React components
  │   │   │   ├── chat/         # Chat UI components (ChatContainer, ChatInput, etc.)
  │   │   │   ├── slides/       # Slide-related components (SlidePreview, etc.)
  │   │   │   ├── layout/       # Layout (Header, Sidebar, etc.)
  │   │   ├── hooks/            # Custom React hooks (useWebSocket, etc.)
  │   │   ├── stores/           # Zustand state stores (ChatStore, SlideStore)
  │   │   ├── lib/              # Utility libraries (api clients, constants)
  │   │   ├── routes/           # Route definitions (pages for chat and slides)
  │   │   └── types/            # Generated API types (api.ts from OpenAPI)
  │   ├── public/               # Static assets (if any, like placeholder images)
  │   ├── package.json, vite.config.js, etc.
  │   └── Dockerfile
  ├── backend/
  │   ├── app/
  │   │   ├── main.py           # FastAPI app instance, includes routes mounting
  │   │   ├── api/              # API route definitions (e.g., chat.py, slides.py, auth.py)
  │   │   ├── models/           # Pydantic models (chat.py, slide.py, etc.)
  │   │   ├── services/         # Service modules (llm.py, images.py, pptx.py)
  │   │   ├── core/             # Core utilities (auth utils, rate limiter, logging config)
  │   │   └── socketio_app.py   # Socket.IO server setup (events for progress)
  │   ├── tests/                # Test cases for backend
  │   ├── requirements.txt and requirements-dev.txt
  │   └── Dockerfile
  ├── docs/
  │   ├── architecture-overview.md
  │   ├── planning/             # Stage 1-5 detailed docs, module specs, etc.
  │   └── README.md
  ├── docker-compose.yml        # Setup to run frontend+backend together
  ├── docker-compose.dev.yml    # Dev-specific overrides (e.g. volumes for live reload)
  ├── .github/
  │   └── workflows/
  │       ├── ci.yml            # CI pipeline definition
  │       └── deploy.yml        # CD pipeline definition
  └── versions.md               # Pinned dependency versions for reference
  ```

  This structure was finalised in Stage 1 and used throughout. It separates concerns clearly and supports the monorepo approach with combined CI.

* **Key Tools and Versions:** The project uses specific versions of frameworks and tools, locked to ensure consistency:

  * *Frontend:* React 18.2.0, TypeScript \~5.8, Vite 5.1.4, @tanstack/react-query 5, @tanstack/router 1, Zustand 4.5, Tailwind CSS 3.4, shadcn UI components (Radix UI v1 under the hood), Framer Motion 10. Testing: Vitest (matching Jest 29 equivalent) and Cypress 12. All these are defined in package.json and devDependencies pinned.
  * *Backend:* Python 3.12.0 runtime, FastAPI 0.111.0, Pydantic 2.3, Uvicorn 0.22, python-socketio 5.8.0. Key libraries: httpx for async HTTP, python-pptx 0.6.21 for PPTX generation, Celery 5.3 (with Redis as broker) for background tasks, PyJWT 2.7 for JWT, and test libs pytest 7.4, pytest-asyncio, respx (for HTTP mocking), schemathesis for contract testing. Lint/format: Ruff 0.0.272, Black 23.9, Mypy for static typing.
  * *Infrastructure/DevOps:* Docker (using Docker Engine 24.x in CI), Kubernetes (k3s 1.27) in staging/production, Helm 3.12 for deployments, Prometheus v2, Grafana 10, Loki 2.8, Tempo 2.1. CI uses GitHub Actions runners (Ubuntu 22.04) with Node 18.x and Python 3.12 as set in the workflow. We pin versions in `versions.md` so that, for example, if a new FastAPI or React comes out, we consciously decide to upgrade rather than auto-update.
  * *Observability & Testing Tools:* k6 0.45 for load testing, OWASP ZAP 2.11 for security scanning, Pact CLI 0. Pactflow/Broker for contract testing if used, Cypress 12 for E2E (running in headless Chrome & Firefox in CI), Lighthouse CI via `treosh/lighthouse-ci-action` for performance budgets, Trivy 0.43 for container scanning.

  All these versions are chosen to be the latest stable as of mid-2025 and are kept constant through development (only updated if a critical fix is needed). By pinning and documenting them, any contributor or new environment can match the exact stack used in development.

* **Quality Gates Recap:** The project enforces several quality gates at different stages of CI (some of which we’ve cited before):

  * Linting must report 0 errors (ESLint, Stylelint, Ruff, Black all pass).
  * Type-checking must pass with 0 errors (TypeScript and mypy).
  * Unit tests must pass with high coverage (90% threshold).
  * Contract tests must pass with no unsatisfied interactions.
  * Performance tests must meet thresholds (fail CI if e.g. P95 > 400ms under test load).
  * Security scans must find no critical issues (we fail the build if Trivy finds a HIGH vuln, or ZAP finds certain alerts).
  * E2E tests in CI must pass reliably (build fails otherwise).
  * Additionally, any Docker image built must pass checks (non-root user check, size check ≤ 250MB for the final image) – Stage 5 tasks included hardening Dockerfiles to meet size and security criteria.

  These gates ensure that at the time of deployment (Stage 5), we have confidence in the product’s quality, and they also serve as **mitigations for risk** as discussed (preventing regressions and catching issues early).

* **Future Enhancements (Post-MVP ideas):** While not part of this PRD’s committed scope, it’s worth noting a few features that are candidates for future development:

  * Adding a **database** (PostgreSQL) and user accounts system to save generated presentations, allow users to revisit or edit them later. This would involve introducing SQLAlchemy models for users, presentations, etc., and likely enabling more persistent session management. (Deferred as noted, since Phases 1–4 explicitly removed DB references to focus on stateless operation first.)
  * More advanced **slide design customisation**: e.g., allowing the user to choose a theme or template for the slides, adjusting colour schemes or font sizes. Technically, this would mean using multiple PowerPoint template files or dynamically styling the PPTX.
  * **Multi-language support**: enabling prompts and presentations in languages other than English. This would require using an LLM model capable of other languages and perhaps localising the UI text.
  * **Integration with Microsoft 365 or Google Slides**: e.g., directly saving the generated presentation to OneDrive/SharePoint or Google Slides instead of a manual download. This would involve using their APIs and likely OAuth flows.
  * **Real-time collaboration**: multiple users refining a presentation together (this would be a large feature, essentially turning it into a live collaborative app with websockets for multi-user editing).
  * **Analytics & Feedback loop**: tracking which slides the user edits after download to feed that back into improving the AI (this is speculative, would require user consent and data pipeline).

  These are beyond MVP and would be considered in later versions once the core product is stable. The current architecture – especially adding a database – has been kept flexible (e.g., no part of the code assumes data *can’t* be stored persistently; we simply short-circuited it for now).

This appendix information is drawn from the supporting documentation and serves as a deeper technical reference for engineering, complementing the main sections of the PRD. It provides specifics that can guide implementation details, ensure alignment with the intended design, and assist future contributors in understanding the system’s components.
